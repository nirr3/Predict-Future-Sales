{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Future Sales\n",
    "### Nicklas Ankarstad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD INTRO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "from itertools import product\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import base\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, KFold, train_test_split\n",
    "import pickle\n",
    "import calendar\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "This contest has several data files that we will have to join together. For those with database modeling understanding, the sales_train file or training set can be thought of as the fact table in a star schema with items, item_ categories and shops being dimensional tables we can join to with a primary key. The test file is another fact with similar relationships. The key difference between the sales_train file and the test file is that the sales file is __daily__ and the test file is __monthly__. This means we will also have to aggregrate daily data to monthly. \n",
    "\n",
    "__sales_train.csv__ - the training set. Daily historical data from January 2013 to October 2015.\n",
    "\n",
    "__test.csv__ - the test set. You need to forecast the sales for these shops and products for November 2015.\n",
    "\n",
    "__sample_submission.csv__ - a sample submission file in the correct format.\n",
    "\n",
    "__items.csv__ - supplemental information about the items/products.\n",
    "\n",
    "__item_categories.csv__  - supplemental information about the items categories.\n",
    "\n",
    "__shops.csv__- supplemental information about the shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## allows us to pick up the european formatting of the dates in the trainset\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%d.%m.%Y') \n",
    "# importing the trainset with dates correctly formatted\n",
    "sales = pd.read_csv('sales_train.csv.gz', parse_dates = ['date'], date_parser = dateparse)\n",
    "#import the rest of the files\n",
    "test = pd.read_csv('test.csv.gz')\n",
    "items = pd.read_csv('items.csv')\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops = pd.read_csv('shops.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have read in all the files. Let's start analyzing them one by one. Starting with the Item Categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Categories file\n",
    "\n",
    "The file is unfortanately in Russian, but based on the names of the columns this file can help us understand what the categories of the products refer to. In this section we will use googletrans to translate the category names to English where-ever possible and then eventually derive a new feature that we will call __Category_type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start by checking out the top five records\n",
    "item_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not speaking Russian can be a disadvantaged, but to the (non-russian) naked eye it appears as though we have a patterns where there is a word followed by PS2, PS3, PS4 and PSP. That looks a lot like it related to various playstation platforms. Maybe the first can help us caetgories these items together. Let's translate the words and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with translating the column item_category name from Russian to English. We will then append that to the original dataframe.\n",
    "translator = Translator()\n",
    "list_a = []\n",
    "for word in item_categories['item_category_name']:\n",
    "    try:\n",
    "        a = translator.translate(word).text\n",
    "        list_a.append(a)\n",
    "    except:\n",
    "        list_a.append(word)\n",
    "item_categories['English_Name'] = list(list_a)\n",
    "    #print(list_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translator is not perfect as it missed some terms. We can manually replace them so our categories are easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Программы means Programs\n",
    "item_categories['English_Name']= item_categories['English_Name'].str.replace(\"Программы\", \"Programs\")\n",
    "\n",
    "## Книги means Books\n",
    "item_categories['English_Name']= item_categories['English_Name'].str.replace(\"Книги\", \"Books\")\n",
    "\n",
    "item_categories.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Looks like the first part of the of each item category has the category type in it.  In our example, the category type was accessories. Let's extract that out and store it in a new feature called Category_type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a feature called Variable type by splitting the English_Name strings where they either have a paranthesis or a dash.\n",
    "list_a = []\n",
    "for row in item_categories['English_Name']:\n",
    "        a = row.replace('(','-').split(' -')[0] ## replacing the opening parantheses with dash so we can use str.split function to split on it.\n",
    "        list_a.append(a)\n",
    "item_categories['Category_type'] = list(list_a)\n",
    "## Lets check out the categories we have\n",
    "pd.DataFrame((item_categories['Category_type'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like several of the categories have similar names and meaning. For example, __game__ console and __gaming__ console are virtually the same type of category. Let's clean those up a bit and get more uniformity of this new feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Let's clean up some of this output in the categories:\n",
    "\n",
    "## Game Consoles are really the same thing as Gaming Consoles\n",
    "item_categories['Category_type']= item_categories['Category_type'].str.replace(\"Gaming Consoles\", \"Game Consoles\")\n",
    "\n",
    "## Payment cards with a lowercase c is the same as Payment Cards with upper case C\n",
    "item_categories['Category_type']= item_categories['Category_type'].str.replace(\"Payment cards\", \"Payment Cards\")\n",
    "\n",
    "## Cinema and movie tends to be synomomous. Let's change \"The Movie\" category type to Cinema\n",
    "item_categories['Category_type']= item_categories['Category_type'].str.replace(\"The Movie\", \"Cinema\")\n",
    "\n",
    "## Pure and Clean Media Seem Similar. Let's combine into Pure/Clean Media\n",
    "item_categories['Category_type']= item_categories['Category_type'].str.replace(\"Clean media\", \"Pure/Clean Media\")\n",
    "item_categories['Category_type']= item_categories['Category_type'].str.replace(\"Pure Media\", \"Pure/Clean Media\")\n",
    "\n",
    "## Lets check out the categories we have\n",
    "pd.DataFrame((item_categories['Category_type'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better. Since this dataset is on the larger side (for a laptop to handle anyways), let's drop the columns we are not going to be using. This will allow us to use less memory on the laptop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets drop item categories_name, the English name and leave only category type\n",
    "\n",
    "item_categories = item_categories.drop(['item_category_name','English_Name'],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. Looks like this dataframe is ready to be used. Let's move on to the shop/store dataframe before we merge all the data together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shops File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the names of the shops. It can be used as a key to get the names for the shop Id's we have in the sales file. Because this file is also in Russian, we will again translate the words into English. Once we have the names in English, we will extract the cities these shops are located within and use that as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite seeing learning about how to spell accesories in Russian, I am afraid my Russian is still not good enough to read the shop names. Let's translate them to English and take a look at what the words mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's translate this into English\n",
    "translator = Translator()\n",
    "list_a = []\n",
    "for word in shops['shop_name']:\n",
    "    a = translator.translate(word).text\n",
    "    list_a.append(a)\n",
    "shops['English_Shop_Name'] = list_a\n",
    "    #print(list_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the city is the first word, followed by something like shopping center, TC or ,SEC. Let's try and extract the city from this. Some googeling of the words made it seem like all the spots I checked, whether they are TC or SEC were in shopping malls. As a result we did not create a featured out that part of the shop name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create city variable with only the city names (first word)\n",
    "## Create a feature called City type by splitting the English_Shops_Name strings where by the spaces. Because there are some cities like St. Petersburg that have a space in their name, we remove spaces following a period and spaces following an exclamation point\n",
    "\n",
    "list_a = []\n",
    "for row in shops['English_Shop_Name']:\n",
    "        a = row.replace('. ','').replace('! ','').split(' ')[0] ## remove spaces follwing period or exclaimation point and split based on spaces. First word is city\n",
    "        list_a.append(a)\n",
    "shops['City'] = list(list_a)\n",
    "## Lets check out the categories we have\n",
    "pd.DataFrame((shops['City'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Are these the same shop?\n",
    "\n",
    "Жуковский ул. Чкалова 39м? and  Жуковский ул. Чкалова 39м²\t \n",
    "Shop_Id 10 and 11\n",
    "\n",
    "!Якутск ТЦ \"Центральный\" фран and Якутск ТЦ \"Центральный\"\t\n",
    "shop ID 01 and 58\n",
    "\n",
    "!Якутск Орджоникидзе, 56 фран and Yakutsk Ordzhonikidze, 56\n",
    "\n",
    "shop ID 0 and 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Filter a down to the two shop_id's we are interested in, group by both the id's and time while adding up all the item_cnt_day\n",
    "## Then plot it\n",
    "\n",
    "## Filter down to the two shops\n",
    "a = sales[(sales['shop_id'] == 10) | (sales['shop_id'] == 11)]\n",
    "## Group and sum the item_cnt_by shop and date block.\n",
    "a = a.groupby(['shop_id','date_block_num']).sum().reset_index()\n",
    "\n",
    "## Seaborne doesn't like when you covert integers and floats to strings and pass them as colors so we add some extra text to trick it to thinking its really a string.\n",
    "\n",
    "rownumb= 0\n",
    "for row in a['shop_id']:\n",
    "    a.loc[rownumb,'shop_id'] = \"shop_id: %r\" % (row)\n",
    "    rownumb = rownumb+1\n",
    "## PLot it.\n",
    "a4_dims = figsize =  (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "#sns.lineplot(x = 'date_block_num', y ='item_cnt_day', hue = 'shop_id', data = a, ci = None, ax = ax)\n",
    "sns.barplot(x = 'date_block_num', y ='item_cnt_day', hue = 'shop_id', data = a, ci = None, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = figsize =  (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "\n",
    "sns.barplot(x = 'date_block_num', y ='item_cnt_day', hue = 'shop_id', data = a, ci = None, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. We can see that for shop 10 had sales for every time block except 25 and the inverse is true for shop 11. This tells us that they are the same. We will replace any reference to 11 with 10 in the sales dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace 11 with 10\n",
    "sales['shop_id'] = sales['shop_id'].replace(11,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Filter a down to the two shop_id's we are interested in, group by both the id's and time while adding up all the item_cnt_day\n",
    "## Then plot it\n",
    "\n",
    "## Filter down to the two shops\n",
    "a = sales[(sales['shop_id'] == 1) | (sales['shop_id'] == 58)]\n",
    "## Group and sum the item_cnt_by shop and date block.\n",
    "a = a.groupby(['shop_id','date_block_num']).sum().reset_index()\n",
    "\n",
    "## Seaborne doesn't like when you covert integers and floats to strings and pass them as colors so we add some extra text to trick it to thinking its really a string.\n",
    "\n",
    "rownumb= 0\n",
    "for row in a['shop_id']:\n",
    "    a.loc[rownumb,'shop_id'] = \"shop_id: %r\" % (row)\n",
    "    rownumb = rownumb+1\n",
    "## PLot it.\n",
    "#sns.lineplot(x = 'date_block_num', y ='item_cnt_day', hue = 'shop_id', data = a, ci = None)\n",
    "\n",
    "\n",
    "a4_dims = figsize =  (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "\n",
    "sns.barplot(x = 'date_block_num', y ='item_cnt_day', hue = 'shop_id', data = a, ci = None, ax = ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we found another. Started as number 1 and continued as number 58. Let's replace 1 with 58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace 1 with 58\n",
    "sales['shop_id'] = sales['shop_id'].replace(1,58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter a down to the two shop_id's we are interested in, group by both the id's and time while adding up all the item_cnt_day\n",
    "## Then plot it\n",
    "\n",
    "## Filter down to the two shops\n",
    "a = sales[(sales['shop_id'] == 0) | (sales['shop_id'] == 59)]\n",
    "## Group and sum the item_cnt_by shop and date block.\n",
    "a = a.groupby(['shop_id','date_block_num']).sum().reset_index()\n",
    "\n",
    "## Seaborne doesn't like when you covert integers and floats to strings and pass them as colors so we add some extra text to trick it to thinking its really a string.\n",
    "\n",
    "rownumb= 0\n",
    "for row in a['shop_id']:\n",
    "    a.loc[rownumb,'shop_id'] = \"shop_id: %r\" % (row)\n",
    "    rownumb = rownumb+1\n",
    "## PLot it.\n",
    "#sns.lineplot(x = 'date_block_num', y ='item_cnt_day', hue = 'shop_id', data = a, ci = None)\n",
    "\n",
    "a4_dims = figsize =  (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "\n",
    "sns.barplot(x = 'date_block_num', y ='item_cnt_day', hue = 'shop_id', data = a, ci = None, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks we have two different shops since shop 0 was having around 3 times as many sales per date block compared to shop 59. Both have sales in the same month as well. Let's leave this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets drop the shop_name, English Shop Name since we extracted the city information from it already\n",
    "shops = shops.drop(['shop_name','English_Shop_Name'],axis = 1)\n",
    "shops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate data\n",
    "Since the competition task is to make a monthly prediction, we need to aggregate the data to monthly level before doing any encodings. The following code-cell serves just that purpose. It also renames the item_cnt_day varibale into Target (once we have made it a monthly aggregate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales[sales['date_block_num']==block_num]['shop_id'].unique()\n",
    "    cur_items = sales[sales['date_block_num']==block_num]['item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "#turn the grid into pandas dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "#get aggregated values for (shop_id, item_id, month)\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "\n",
    "#fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "#join aggregated data to the grid\n",
    "all_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n",
    "#sort the data\n",
    "all_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another key component of this competition is the need to set a maximum of 20 for each monthly target. This means that if we see a value larger than 20, we will automatically call it 20. This has a significant positive impact on our RMSE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['target']=all_data['target'].clip(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create one dataframe with our train and test sets. We will join this with the item_categories, items and shops. We will use this dataframe to create alot of our features and reduce the need to apply them to multiple dataframes wherever possible. For example, when we create lagged variables, we need to create them for both the train, validation and test sets. By combining all the data into one dataframe we only have to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's check out the shape of our datasets\n",
    "all_data.shape , test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will union train and test sets together. As we saw from the code right above, we are missing two columns on the test set. These are date_block_num and target. For now we will assign the target to be zero. We will also assign the number 34 to the date_block_num. The date_block_num corresponds to the month in the dataset, so since we need to predict next months item_counts, we will simply look for the max of the training set and add one. (the max is 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Assign 34 to date_block_num and 0.0 to target\n",
    "test['date_block_num'] = 34\n",
    "test['target'] = 0.0\n",
    "\n",
    "TEST_ID = test['ID'] ## in case we need this later\n",
    "\n",
    "## Then we need to union them and save that back as our all_data dataframe\n",
    "all_data = pd.concat([all_data,test], axis =0, sort=True)\n",
    "all_data = all_data.drop(columns = ['ID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will merge the all_data dataframe with the items, item_categories and shops dataframes. Since we want to avoid kartesian products so we will add a number of row counts checker to make sure we do not add any new rows or drop any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate number of rows prior to merge\n",
    "prior_rows = all_data.shape[0]\n",
    "\n",
    "## Merge the sales train data with the items, item categoris and shops datasets to get the names of items, their categories and the shop names\n",
    "all_data = pd.merge(all_data, items, on = \"item_id\")\n",
    "all_data = pd.merge(all_data, item_categories, on = \"item_category_id\")\n",
    "all_data = pd.merge(all_data, shops, on = \"shop_id\")\n",
    "\n",
    "## Calcualte number and print of rows dropped (should be zero)\n",
    "print(\"Dropped {} rows\".format(prior_rows - all_data.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import calendar\n",
    "# from datetime import datetime\n",
    "\n",
    "# len([1 for i in calendar.monthcalendar(datetime.now().year,\n",
    "#                                   datetime.now().month) if i[6] != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates\n",
    "\n",
    "Dates can tell us a lot of things about sales. For example, February sales may be lower than January sales simply because February has fewer days in it (ie less time to sell). The types of days also matter. More weekend days may mean that more people frequent the stores. Seasons matter as well, where June sales may be different than Decembers. We will create features related to all of these items.\n",
    "\n",
    "To get started, we first need to extract the Month-End Dates for each dateblock and store it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Pull out the last date of each dateblock and append it to the \n",
    "\n",
    "list_a = []\n",
    "for dateblock in sales['date_block_num'].unique():\n",
    "    a = sales[sales['date_block_num'] == dateblock]\n",
    "    a = max(a['date'])\n",
    "    list_a.append(a)\n",
    "    \n",
    "list_a.append(datetime.strptime('2015-11-30','%Y-%m-%d')) ## Manually adding the month for the test set\n",
    "## Transform it to dataframe so we can merge with all_data\n",
    "list_a = pd.DataFrame(list_a)\n",
    "## Give the data a descriptive column header\n",
    "list_a.columns = ['Month_End_Date'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Month End Dates have been extracted, we can count the number of mondays, tuesdays etc there were in each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Let's calculate the number of specific days are in each month.\n",
    "\n",
    "## Create the empty lists\n",
    "mon_list = []\n",
    "tue_list = []\n",
    "wed_list = []\n",
    "thu_list = []\n",
    "fri_list = []\n",
    "sat_list = []\n",
    "sun_list = []\n",
    "\n",
    "## Calculate the number of a specific day in a given month (for example, number of mondays in March of 2015)\n",
    "for date in list_a['Month_End_Date']:\n",
    "    mon_list.append((len([1 for i in calendar.monthcalendar(date.year,date.month) if i[0] != 0])))\n",
    "    tue_list.append((len([1 for i in calendar.monthcalendar(date.year,date.month) if i[1] != 0])))\n",
    "    wed_list.append((len([1 for i in calendar.monthcalendar(date.year,date.month) if i[2] != 0])))\n",
    "    thu_list.append((len([1 for i in calendar.monthcalendar(date.year,date.month) if i[3] != 0])))\n",
    "    fri_list.append((len([1 for i in calendar.monthcalendar(date.year,date.month) if i[4] != 0])))\n",
    "    sat_list.append((len([1 for i in calendar.monthcalendar(date.year,date.month) if i[5] != 0])))\n",
    "    sun_list.append((len([1 for i in calendar.monthcalendar(date.year,date.month) if i[6] != 0])))\n",
    "\n",
    "## Add these to our list we created with the dates\n",
    "list_a['Number_of_Mondays'] = mon_list\n",
    "list_a['Number_of_Tuesdays'] = tue_list\n",
    "list_a['Number_of_Wednesdays'] = wed_list\n",
    "list_a['Number_of_Thursdays'] = thu_list\n",
    "list_a['Number_of_Fridays'] = fri_list\n",
    "list_a['Number_of_Saturdays'] = sat_list\n",
    "list_a['Number_of_Sundays'] = sun_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract features related to the year, the month and the number of days in the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create the empty lists again\n",
    "\n",
    "year_list = []\n",
    "month_list = []\n",
    "day_list = []\n",
    "\n",
    "## Next lets calculate strip out the number of days in a month, the number of the month and the number of the year\n",
    "for date in list_a['Month_End_Date']:\n",
    "    year_list.append(date.year)\n",
    "    month_list.append(date.month)\n",
    "    day_list.append(date.day)\n",
    "\n",
    "## Add to our dataframe\n",
    "list_a['Year'] = year_list\n",
    "list_a['Month'] = month_list\n",
    "list_a['Days_in_Month'] = day_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice date dataframe called list_a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list_a dataframe can be merged back with the all_data dataframe and we've added a couple of date features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Merge the new dataframe with the all_data, using the index and the date_block_num as keys\n",
    "all_data = pd.merge(all_data, list_a, left_on = 'date_block_num', right_index = True)\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price\n",
    "\n",
    "Our original approach was to add the count of transactions and we did not do anything with Price of the item. Let's average the monthly price and merge that feature back with our all_data dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding the average monthly price within a monthly block for each item at each store to the dataset\n",
    "a = sales.groupby(['date_block_num','shop_id','item_id'])['item_price'].mean()\n",
    "a = pd.DataFrame(a)\n",
    "all_data = pd.merge(all_data,a,how = \"left\", left_on = ['date_block_num','shop_id','item_id'], right_on = ['date_block_num','shop_id','item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Months Since Item First Sold & Months Since Item was Last Sold\n",
    "\n",
    "These features show the number of date blocks (months) since the first time the item was sold and the last time the item was sold. These will help us understand how new the item is and could potentially tell us that the item is no longer being sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Let's calculate how \"fresh\" the items are. We will calculate the min for each item. This will give us the first month it was sold in\n",
    "### Then we will calculate the difference between that number and the current date block to see how \"old\" the item is.\n",
    "a = all_data.groupby('item_id')['date_block_num'].min()\n",
    "a = pd.DataFrame(a)\n",
    "a = a.reset_index()\n",
    "a.columns = ['item_id','min_item_sale_date_block_num']\n",
    "all_data = pd.merge(all_data,a, left_on = 'item_id', right_on = 'item_id')\n",
    "all_data['Months_Since_Item_First_Sold'] = all_data['date_block_num']- all_data['min_item_sale_date_block_num']\n",
    "\n",
    "\n",
    "# ### Let's calculate how \"stale\" the items are. We will calculate the max for each item. This will give us the first month it was sold in\n",
    "# ### Then we will calculate the difference between that number and the current date block to see how \"old\" the item is.\n",
    "# a = all_data.groupby('item_id')['date_block_num'].max()\n",
    "# a = pd.DataFrame(a)\n",
    "# a = a.reset_index()\n",
    "# a.columns = ['item_id','max_item_sale_date_block_num']\n",
    "# all_data = pd.merge(all_data,a, left_on = 'item_id', right_on = 'item_id')\n",
    "# all_data['Months_Since_Item_Last_Sold'] = all_data['date_block_num']- all_data['max_item_sale_date_block_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data in the test set are for products we have never seen before. Lets create a features that calculated the average monthly sales for a specific item only in the first month it was sold. We will make the rest of them zero. \n",
    "\n",
    "We will also apply the same logic to item categories and shop ids combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the average sales in the first month by category\n",
    "a = all_data[all_data['Months_Since_Item_First_Sold'] == 0].groupby(['item_category_id','Months_Since_Item_First_Sold'])['target'].mean()\n",
    "a = pd.DataFrame(a)\n",
    "a = a.reset_index()\n",
    "a.columns = ['item_category_id','Months_Since_Item_First_Sold','avg_first_months_sales_by_item_category_id']\n",
    "all_data = pd.merge(all_data,a, left_on = ['item_category_id','Months_Since_Item_First_Sold'], right_on = ['item_category_id','Months_Since_Item_First_Sold'], how = 'left')\n",
    "all_data['avg_first_months_sales_by_item_category_id'] = all_data['avg_first_months_sales_by_item_category_id'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## calculate the average sales in the first month by category and shop ID\n",
    "a = all_data[all_data['Months_Since_Item_First_Sold'] == 0].groupby(['item_category_id', 'Months_Since_Item_First_Sold','shop_id'])['target'].mean()\n",
    "a = pd.DataFrame(a)\n",
    "a = a.reset_index()\n",
    "a.columns = ['item_category_id','Months_Since_Item_First_Sold','shop_id','avg_first_months_sales_by_item_category_and_shop']\n",
    "all_data = pd.merge(all_data,a, left_on = ['item_category_id','Months_Since_Item_First_Sold','shop_id'], right_on = ['item_category_id','Months_Since_Item_First_Sold', 'shop_id'], how = 'left')\n",
    "all_data['avg_first_months_sales_by_item_category_and_shop'] = all_data['avg_first_months_sales_by_item_category_and_shop'].fillna(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## calculate the average sales in the first month by category\n",
    "# a = all_data[all_data['Months_Since_Item_First_Sold'] == 0].groupby(['item_category_id','Months_Since_Item_First_Sold'])['target'].mean()\n",
    "# a = pd.DataFrame(a)\n",
    "# a = a.reset_index()\n",
    "# a.columns = ['item_category_id','Months_Since_Item_First_Sold','avg_first_months_sales_by_item_category_id']\n",
    "# all_data = pd.merge(all_data,a, left_on = ['item_category_id','Months_Since_Item_First_Sold'], right_on = ['item_category_id','Months_Since_Item_First_Sold'], how = 'left')\n",
    "# all_data['avg_first_months_sales_by_item_category_and_shop'] = all_data['avg_first_months_sales_by_item_category_and_shop'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data['Months_Since_Item_First_Sold'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagged Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I was only allowed one datapoint to predict what next month's sales would be, I would probably use this months sales. This month's sale is a lagged 1 variable. Lags of target variables are very common in Time Series (Luckily we are not limited to only one data point). We will create a couple of lagged variables (ie last months sales, last (last) month's sales etc. We will repead this for several months. \n",
    "\n",
    "To set up the lagging features we will use a helper function (kindly provided by https://www.kaggle.com/dlarionov/feature-engineering-xgboost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a helper function that allow us to pass what dataset we want to perform the lag on (df), \n",
    "## the lags we want to do and the column we want to lag\n",
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function we just created, we will lag the target 1,2,3,4,5,6 and 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "all_data = lag_feature(all_data, [1,2,3,4,5,6,12], 'target')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE! Lagging is fun. Let's create features using the mean for each month and either category, city, shop or item. We can use the lags of these features in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average number of sales by month and by item\n",
    "all_data['avg_monthly_by_item'] = all_data.groupby(['item_id', 'date_block_num'])['target'].transform('mean')\n",
    "\n",
    "## Average number of sales by month and by shop\n",
    "all_data['avg_monthly_by_shop'] = all_data.groupby(['shop_id', 'date_block_num'])['target'].transform('mean')\n",
    "\n",
    "## Average number of sales by month and by category\n",
    "all_data['avg_monthly_by_category'] = all_data.groupby(['Category_type', 'date_block_num'])['target'].transform('mean')\n",
    "\n",
    "## Average number of sales by month and by city\n",
    "all_data['avg_monthly_by_city'] = all_data.groupby(['City', 'date_block_num'])['target'].transform('mean')\n",
    "\n",
    "\n",
    "\n",
    "### Let's lag the variables we just created using the lag_feature helper function. We will also lag the sale price since its not in the test set.\n",
    "\n",
    "ts = time.time()\n",
    "all_data = lag_feature(all_data, [1,2,3,6,12], 'avg_monthly_by_item')\n",
    "all_data = lag_feature(all_data, [1,2,3,6,12], 'avg_monthly_by_shop')\n",
    "all_data = lag_feature(all_data, [1,2,3,6,12], 'avg_monthly_by_category')\n",
    "all_data = lag_feature(all_data, [1,2,3,6,12], 'avg_monthly_by_city')\n",
    "all_data = lag_feature(all_data, [1,2,3,4,5,6], 'item_price')\n",
    "time.time() - ts\n",
    "\n",
    "## Dropping these as they would be NA's for the test set.\n",
    "all_data = all_data.drop(['avg_monthly_by_item','avg_monthly_by_shop','avg_monthly_by_category','avg_monthly_by_city','item_price'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lagged variables had a lot of Nulls as we didn't always have a sale in prior months. Let's make check how many we have before we assign them all to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "null_list = all_data.isnull().sum(axis=0) ## sum of all the nulls. Store them in a pandas series object\n",
    "null_df = pd.DataFrame(null_list) ## Convert to data frame so we can easily filter\n",
    "null_df[null_df[0] > 0] ## Show only the columns with nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like its only the lagged variables. A Null here really means we had zero sales. So lets make them Zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in all_data:\n",
    "    all_data[column] = all_data[column].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of interest is rolling or moving averages. Let's create two rolling averages, 3 and 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate rolling average\n",
    "all_data['target_3_month_avg'] = (all_data['target_lag_1'] + all_data['target_lag_2'] +all_data['target_lag_3']) /3\n",
    "all_data['target_6_month_avg'] = (all_data['target_lag_1'] + all_data['target_lag_2'] +all_data['target_lag_3'] + all_data['target_lag_4'] + all_data['target_lag_5'] +all_data['target_lag_6']) /6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that is a lot of features we have created.  Since I am only working with a laptop, we should probably make sure we are storing this in as small of dataframe as possible. Let's check the memory usage and the datatypes of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_data.info(memory_usage = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of these are listed as either int64 or float64, we can probably reduce them down to smaller space datatypes like int16 or float8. Downcasting means we reduce the datatypes of each feature to its lowest possible type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in all_data:\n",
    "    if all_data[column].dtype == 'float64':\n",
    "        all_data[column]=pd.to_numeric(all_data[column], downcast='float')\n",
    "    if all_data[column].dtype == 'int64':\n",
    "        all_data[column]=pd.to_numeric(all_data[column], downcast='integer')\n",
    "## Dropping Item name to free up memory\n",
    "all_data = all_data.drop('item_name',axis =1)\n",
    "## Let's check the size\n",
    "all_data.info(memory_usage = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been downcast, we can start splitting data into training (first 32 months), validation (month 33) and back to our test set (month 34). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_data[all_data.date_block_num < 33]\n",
    "Y_train = all_data[all_data.date_block_num < 33]['target']\n",
    "X_valid = all_data[all_data.date_block_num == 33]\n",
    "Y_valid = all_data[all_data.date_block_num == 33]['target']\n",
    "X_test = all_data[all_data.date_block_num == 34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding\n",
    "\n",
    "### Why do we target encode? \n",
    "\n",
    "Gradient boosted tree-based models such as XGBoost and LightGBM have a hard time handling high cardinality categorical variables. Target Encoding helps with that and can help improve the model performance. \n",
    "\n",
    "### Why do we need to reguralize?\n",
    "\n",
    "Simply calculating the averages of the target variables can cause overfitting and often reduces the models ability to be generalized. \n",
    "\n",
    "#### Regularization Techniques:\n",
    "\n",
    " - Cross Validation Loop inside training data\n",
    " - Smoothing\n",
    " - Adding Random Noise\n",
    " - Sorting and Calculating expanding mean\n",
    "\n",
    "We will only do cross-validation Loop inside our training data. To get started, we will define two helper function that I picked up from https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpder Function to KFold Mean encoding\n",
    "class KFoldTargetEncoderTrain(base.BaseEstimator,\n",
    "                               base.TransformerMixin):\n",
    "    def __init__(self,colnames,targetName,\n",
    "                  n_fold=5, verbosity=True,\n",
    "                  discardOriginal_col=False):\n",
    "        self.colnames = colnames\n",
    "        self.targetName = targetName\n",
    "        self.n_fold = n_fold\n",
    "        self.verbosity = verbosity\n",
    "        self.discardOriginal_col = discardOriginal_col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        assert(type(self.targetName) == str)\n",
    "        assert(type(self.colnames) == str)\n",
    "        assert(self.colnames in X.columns)\n",
    "        assert(self.targetName in X.columns)\n",
    "        mean_of_target = X[self.targetName].mean()\n",
    "        kf = KFold(n_splits = self.n_fold,\n",
    "                   shuffle = False, random_state=2019)\n",
    "        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n",
    "        X[col_mean_name] = np.nan\n",
    "        for tr_ind, val_ind in kf.split(X):\n",
    "            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n",
    "            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)[self.targetName].mean())\n",
    "            X[col_mean_name].fillna(mean_of_target, inplace = True)\n",
    "        if self.verbosity:\n",
    "            encoded_feature = X[col_mean_name].values\n",
    "            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,self.targetName,                    \n",
    "                   np.corrcoef(X[self.targetName].values,\n",
    "                               encoded_feature)[0][1]))\n",
    "        if self.discardOriginal_col:\n",
    "            X = X.drop(self.targetName, axis=1)\n",
    "        return X\n",
    "\n",
    "    \n",
    "## Helper function to get the Kfold Mean encoded on the test set\n",
    "\n",
    "class KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,train,colNames,encodedName):\n",
    "        \n",
    "        self.train = train\n",
    "        self.colNames = colNames\n",
    "        self.encodedName = encodedName\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        mean =  self.train[[self.colNames,\n",
    "                self.encodedName]].groupby(\n",
    "                                self.colNames).mean().reset_index() \n",
    "        \n",
    "        dd = {}\n",
    "        for index, row in mean.iterrows():\n",
    "            dd[row[self.colNames]] = row[self.encodedName]\n",
    "        X[self.encodedName] = X[self.colNames]\n",
    "        X = X.replace({self.encodedName: dd})\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both of our helper functions defined, lets use them to start mean-encoding our variables:\n",
    "- item_id\n",
    "- shop_id\n",
    "- City\n",
    "- Category_type\n",
    "- item_category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Transforms the train set with a mean-encoded feature\n",
    "\n",
    "## item_id mean encoding\n",
    "targetc = KFoldTargetEncoderTrain('item_id','target',n_fold=5)\n",
    "X_train = targetc.fit_transform(X_train)\n",
    "\n",
    "## shop_id mean encoding\n",
    "targetc = KFoldTargetEncoderTrain('shop_id','target',n_fold=5)\n",
    "X_train = targetc.fit_transform(X_train)\n",
    "\n",
    "## City mean encoding\n",
    "targetc = KFoldTargetEncoderTrain('City','target',n_fold=5)\n",
    "X_train = targetc.fit_transform(X_train)\n",
    "\n",
    "## Category_type mean encoding\n",
    "targetc = KFoldTargetEncoderTrain('Category_type','target',n_fold=5)\n",
    "X_train = targetc.fit_transform(X_train)\n",
    "\n",
    "\n",
    "## Item_category_id mean encoding\n",
    "targetc = KFoldTargetEncoderTrain('item_category_id','target',n_fold=5)\n",
    "X_train = targetc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform validation & test set\n",
    "\n",
    "## Apply item id mean encoding to test set\n",
    "test_targetc = KFoldTargetEncoderTest(X_train,'item_id','item_id_Kfold_Target_Enc')\n",
    "X_valid = test_targetc.fit_transform(X_valid)\n",
    "X_test = test_targetc.fit_transform(X_test)\n",
    "\n",
    "## Apply shop id mean encoding to test set\n",
    "test_targetc = KFoldTargetEncoderTest(X_train,'shop_id','shop_id_Kfold_Target_Enc')\n",
    "X_valid = test_targetc.fit_transform(X_valid)\n",
    "X_test = test_targetc.fit_transform(X_test)\n",
    "\n",
    "\n",
    "## Apply city mean encoding to test set\n",
    "test_targetc = KFoldTargetEncoderTest(X_train,'City','City_Kfold_Target_Enc')\n",
    "X_valid = test_targetc.fit_transform(X_valid)\n",
    "X_test = test_targetc.fit_transform(X_test)\n",
    "\n",
    "## Apply Category_type mean encoding to test set\n",
    "test_targetc = KFoldTargetEncoderTest(X_train,'Category_type','Category_type_Kfold_Target_Enc')\n",
    "X_valid = test_targetc.fit_transform(X_valid)\n",
    "X_test = test_targetc.fit_transform(X_test)\n",
    "\n",
    "## Apply item_category_id mean encoding to test set\n",
    "test_targetc = KFoldTargetEncoderTest(X_train,'item_category_id','item_category_id_Kfold_Target_Enc')\n",
    "X_valid = test_targetc.fit_transform(X_valid)\n",
    "X_test = test_targetc.fit_transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Dataset\n",
    "\n",
    "We are getting close. Our fetures are done. Let' do a couple of checks to make sure we only have the features we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## drop first 12 months since we have lagged variables\n",
    "X_train = X_train[X_train.date_block_num > 12]\n",
    "\n",
    "## Assign target variables to seperate variables\n",
    "y= X_train['target']\n",
    "Y_valid = X_valid['target']\n",
    "\n",
    "\n",
    "## Drop Categorical Variables that we mean encoded, the target and the item codes.\n",
    "columns_to_drop = ['target', 'Category_type','City','Month_End_Date', 'item_category_id']\n",
    "X_train= X_train.drop(columns_to_drop, axis = 1)\n",
    "X_valid = X_valid.drop(columns_to_drop, axis = 1)\n",
    "X_test = X_test.drop(columns_to_drop, axis = 1)\n",
    "\n",
    "\n",
    "## Double check that our shapes are the same\n",
    "X_train.shape,X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the files to a local drive using Pickle\n",
    "\n",
    "As I mentioned before, memory is an issue. Here is an option to save the dataframes we have used thus far using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'X_train'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(X_train,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Y_train'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(y,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'X_valid'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(X_valid,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'X_test'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(X_test,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Y_valid'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(Y_valid,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'all_data'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(all_data,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the pickled files in again\n",
    "\n",
    "Since we have saved off the files to our local directory, we can restart the kernel if needed and just bring in the dataframes we need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'X_train'\n",
    "infile = open(filename,'rb')\n",
    "X_train = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'X_valid'\n",
    "infile = open(filename,'rb')\n",
    "X_valid = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Y_train'\n",
    "infile = open(filename,'rb')\n",
    "y = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Y_valid'\n",
    "infile = open(filename,'rb')\n",
    "Y_valid = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'X_test'\n",
    "infile = open(filename,'rb')\n",
    "X_test = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For whatever reason the test data got shuffled. So we will read the test file in again, merge it with the X_test data\n",
    "## to get the proper order of the ID's\n",
    "test = pd.read_csv('test.csv.gz')\n",
    "TEST_ID = test['ID']\n",
    "a = pd.merge(X_test, test, left_on=['shop_id','item_id'],right_on=['shop_id','item_id'])\n",
    "test_id_to_be_used= a['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "- Faster training speed and higher efficiency.\n",
    "- Lower memory usage.\n",
    "- Better accuracy.\n",
    "- Support of parallel and GPU learning.\n",
    "- Capable of handling large-scale data.\n",
    "- For more details, please refer to Features.\n",
    "\n",
    "\n",
    "LightGBM really good at handling datasets larger than 100K records, and does so realatively fast compared to XGBoost. \n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform train and validation into lgb dataset structures required for modeling.\n",
    "lgb_train = lgb.Dataset(X_train, y)\n",
    "lgb_eval = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most boosted models, we will need to tune our hyper-paremeters. These are the ones that I had the most success with, but it does not mean that they are the \"ultimate\" ones.\n",
    "\n",
    "Generally speaking, I follow the guidance of the documentations:\n",
    "\n",
    "LightGBM uses the leaf-wise tree growth algorithm, while many other popular tools use depth-wise tree growth. Compared with depth-wise growth, the leaf-wise algorithm can converge much faster. However, the leaf-wise growth may be over-fitting if not used with the appropriate parameters.\n",
    "\n",
    "To get good results using a leaf-wise tree, these are some important parameters:\n",
    "\n",
    "__num_leaves.__ This is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.\n",
    "\n",
    "__min_data_in_leaf.__ This is a very important parameter to prevent over-fitting in a leaf-wise tree. Its optimal value depends on the number of training samples and num_leaves. Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.\n",
    "max_depth. You also can use max_depth to limit the tree depth explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'num_threads' : 4\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=100)\n",
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(Y_valid, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_features = 50\n",
    "indxs = np.argsort(gbm.feature_importance())[:num_features]\n",
    "    \n",
    "feature_imp = pd.DataFrame(sorted(zip(gbm.feature_importance()[indxs],X_train.columns[indxs])), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('Top {} LightGBM Features accorss folds'.format(num_features))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "Y_pred = Y_pred.clip(0,20)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_id_to_be_used, \n",
    "    \"item_cnt_month\": Y_pred\n",
    "})\n",
    "submission.to_csv('lgb_submission 7.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading this far! If you like what you saw, please upvote or comment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "\n",
    "model = XGBRegressor(\n",
    " #   max_depth=3,\n",
    " #   n_estimators=1000,\n",
    " #   min_child_weight=300, \n",
    "  #  colsample_bytree=0.8, \n",
    "  #  subsample=0.8, \n",
    "  eta=0.3,    \n",
    "  #  seed=42,\n",
    "    nthreads = 4)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, y), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 20)\n",
    "\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = Y_pred.clip(0,20)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_id_to_be_used, \n",
    "    \"item_cnt_month\": Y_pred\n",
    "})\n",
    "submission.to_csv('xgb_submission v7.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining error functions for handy use. \n",
    "\n",
    "kfolds = KFold(n_splits=10, shuffle=False, random_state=4202)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model, X=X_train):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds, tol=0.001))\n",
    "elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \n",
    "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                                       num_leaves=4,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       feature_fraction=0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=-1,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     objective='reg:linear', nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge RMSE: 0.8254 STD: (0.3101) Time: 3:22:23.111299\n",
      "lasso RMSE: 0.8255 STD: (0.3103) Time: 7:12:27.014321\n",
      "elasticnet RMSE: 0.8254 STD: (0.3103) Time: 8:06:15.958091\n",
      "xgboost RMSE: 0.8197 STD: (0.3244) Time: 2 days, 21:15:18.372290\n",
      "[07:19:42] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[23:05:37] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[14:46:16] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[06:30:19] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[22:16:47] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[14:00:08] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[05:44:10] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[21:31:19] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[13:14:34] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[04:59:34] Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "lightgbm RMSE: 0.7987 STD: (0.3073) Time: 6 days, 13:11:58.115774\n"
     ]
    }
   ],
   "source": [
    "## Iterate over each of the model, calculate Average RMSE, STD of RMSE and time to run\n",
    "\n",
    "models = (ridge, lasso, elasticnet, gbr, xgboost, lightgbm)\n",
    "models_str =('ridge', 'lasso', 'elasticnet', 'xgboost', 'lightgbm')\n",
    "for model_name,model in zip(models_str, models):    \n",
    "    start_time= datetime.now()\n",
    "    score = cv_rmse(model , X_train)\n",
    "    print(\"{} RMSE: {:.4f} STD: ({:.4f}) Time:\".format(str(model_name),score.mean(), score.std()), datetime.now()-start_time, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START Fit\n",
      "stack_gen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('START Fit')\n",
    "\n",
    "print('stack_gen')\n",
    "stack_gen_model = stack_gen.fit(np.array(X_train), np.array(y))\n",
    "\n",
    "print('elasticnet')\n",
    "elastic_model_full_data = elasticnet.fit(X_train, y)\n",
    "\n",
    "print('Lasso')\n",
    "lasso_model_full_data = lasso.fit(X_train, y)\n",
    "\n",
    "print('Ridge')\n",
    "ridge_model_full_data = ridge.fit(X_train, y)\n",
    "\n",
    "\n",
    "print('xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X_train,y)\n",
    "\n",
    "print('lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_models_predict(X):\n",
    "    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n",
    "            (0.1 * lasso_model_full_data.predict(X)) + \\\n",
    "            (0.1 * ridge_model_full_data.predict(X)) + \\\n",
    "#            (0.1 * gbr_model_full_data.predict(X)) + \\\n",
    "            (0.2 * xgb_model_full_data.predict(X)) + \\\n",
    "            (0.2 * lgb_model_full_data.predict(X)) + \\\n",
    "            (0.3 * stack_gen_model.predict(np.array(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSLE score on validation data:')\n",
    "print(rmsle(Y_valid, blend_models_predict(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = blend_models_predict(X_test)\n",
    "Y_pred = Y_pred.clip(0,20)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_id_to_be_used, \n",
    "    \"item_cnt_month\": Y_pred\n",
    "})\n",
    "submission.to_csv('blended_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    " - Test SET\n",
    "     - There are items that are new in the test set. How to handle?\n",
    "     \n",
    " - What about stale items? Should I just set them to zero if they haven't had a sale in say 6 months?\n",
    " \n",
    " \n",
    " - Run the Ensemble models\n",
    " \n",
    " - Add Features:\n",
    " - Discounts\n",
    " - Shop Revenue\n",
    " \n",
    " - \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'all_data'\n",
    "infile = open(filename,'rb')\n",
    "all_data = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## allows us to pick up the european formatting of the dates in the trainset\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%d.%m.%Y') \n",
    "# importing the trainset with dates correctly formatted\n",
    "sales = pd.read_csv('sales_train.csv.gz', parse_dates = ['date'], date_parser = dateparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = sales.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "all_data = pd.merge(all_data, group, on=['item_id'], how='left')\n",
    "all_data['item_avg_item_price'] = all_data['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "group = sales.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "all_data = pd.merge(all_data, group, on=['date_block_num','item_id'], how='left')\n",
    "all_data['date_item_avg_item_price'] = all_data['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "lags = [1,2,3,4,5,6]\n",
    "all_data = lag_feature(all_data, lags, 'date_item_avg_item_price')\n",
    "\n",
    "for i in lags:\n",
    "    all_data['delta_price_lag_'+str(i)] = (all_data['date_item_avg_item_price_lag_'+str(i)] - all_data['item_avg_item_price']) / all_data['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if row['delta_price_lag_'+str(i)]:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0\n",
    "    \n",
    "all_data['delta_price_lag'] = all_data.apply(select_trend, axis=1)\n",
    "all_data['delta_price_lag'] = all_data['delta_price_lag'].astype(np.float16)\n",
    "all_data['delta_price_lag'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From:\n",
    "\n",
    "## https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data\n",
    "no_data_items = X_test[~(X_test['item_id'].isin(X_train['item_id']))]\n",
    "len(no_data_items['item_id'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
